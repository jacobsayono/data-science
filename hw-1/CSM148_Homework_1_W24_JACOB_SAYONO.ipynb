{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jacob Sayono\n",
    "\n",
    "505368811\n",
    "\n",
    "CS M148 Winter 2024\n",
    "\n",
    "Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data & Bias\n",
    "\n",
    "## Part (a)\n",
    "\n",
    "Yes, my friend's data collection method exhibit selection bias because the data collection method is not random and does not represent the entire student population's opinions. Reddit users represent a specific segment of the student body, likely those who are more tech-savvy or inclined to share their experiences online. This group may not accurately reflect the diversity of opinions and experiences of all students regarding the dining hall menus. Furthermore, the sentiment analysis model's ability to interpret nuances and the context of the text may also introduce bias, depending on its training data and algorithms. Therefore, this method is likely to yield skewed insights that do not fully capture the broader student body's sentiments.\n",
    "\n",
    "## Part (b)\n",
    "\n",
    "The discrimination against women by AI recruiters can be attributed to the training data used to develop these systems. If the historical hiring data reflects a bias towards selecting male candidates over female candidates, the AI will learn and perpetuate this bias. Removing gender data from the dataset might reduce direct gender discrimination, but it does not eliminate bias. Other variables correlated with gender, such as gaps in employment history (which might occur due to maternity leave) or differences in job titles and industries where one gender may dominate, can still lead to biased outcomes. Therefore, merely dropping the gender variable does not address the underlying issue of biased training data and the AI system's ability to infer gender through correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression: goodness of fit & Interpretation\n",
    "\n",
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: β0 = -2732.678350515464, β1 = 1.4907216494845361\n",
      "Predicted population for 2010: 263.6721649484534 million\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "years = np.array([1820, 1870, 1910, 1950, 2000])\n",
    "population = np.array([9, 40, 92, 151, 281])\n",
    "\n",
    "# Mean of x and y\n",
    "x_mean = np.mean(years)\n",
    "y_mean = np.mean(population)\n",
    "\n",
    "# Calculate β1\n",
    "beta_1 = np.sum((years - x_mean) * (population - y_mean)) / np.sum((years - x_mean) ** 2)\n",
    "\n",
    "# Calculate β0\n",
    "beta_0 = y_mean - beta_1 * x_mean\n",
    "\n",
    "# Predict population for 2010\n",
    "year_2010 = 2010\n",
    "population_2010 = beta_0 + beta_1 * year_2010\n",
    "\n",
    "print(f\"Model parameters: β0 = {beta_0}, β1 = {beta_1}\")\n",
    "print(f\"Predicted population for 2010: {population_2010} million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 for the model: 0.9323216115302541\n"
     ]
    }
   ],
   "source": [
    "# Calculate SS_res and SS_tot\n",
    "SS_res = np.sum((population - (beta_0 + beta_1 * years)) ** 2)\n",
    "SS_tot = np.sum((population - y_mean) ** 2)\n",
    "\n",
    "# Calculate R^2\n",
    "R_squared = 1 - SS_res / SS_tot\n",
    "\n",
    "print(f\"R^2 for the model: {R_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes because R^2 is close to 1, suggesting that the regression line fits the data well. Conversely, a lower R^2 value would indicate that the model does not explain much of the variability in the population data, suggesting a poor fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = population - (beta_0 + beta_1 * years)\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(years, residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. There's a clear pattern (e.g., a curve or increasing/decreasing trend), this indicates that the model may not be capturing all the underlying trends in the data, and a non-linear model might be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "The plot shows a strong correlation between heart disease and wine consumption. Based on the provided information, there is a statistically significant linear relationship between heart disease deaths and wine consumption, as evidenced by the regression line and the R-squared value of 71.0%.\n",
    "\n",
    "However, correlation does not imply causation. While the data suggests a correlation between higher wine consumption and lower heart disease deaths, it does not necessarily mean that increasing wine consumption will directly reduce the risk of heart disease. There could be other factors at play that are not accounted for in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (Consumption based on Income): β0 = 4.270916376452803, β1 = 0.6199702093735578\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Data\n",
    "income = np.array([100, 90, 28, 40, 39, 59, 60, 62, 64, 100, 104, 110, 114, 132]).reshape(-1, 1)\n",
    "consumption = np.array([27, 41, 50, 15, 20, 28, 43, 50, 51, 75, 83, 80, 85, 95])\n",
    "experience = np.array([8, 9, 2, 3, 1, 3, 5, 3, 5, 7, 10, 12, 4, 8])\n",
    "\n",
    "# Model for consumption based on income\n",
    "model_consumption_income = LinearRegression().fit(income, consumption)\n",
    "beta_0_ci = model_consumption_income.intercept_\n",
    "beta_1_ci = model_consumption_income.coef_[0]\n",
    "\n",
    "# Print the coefficients for model 1\n",
    "print(f\"Model 1 (Consumption based on Income): β0 = {beta_0_ci}, β1 = {beta_1_ci}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 (Income based on Working Experience): β0 = [35.4], β1 = [7.58]\n"
     ]
    }
   ],
   "source": [
    "# Model for income based on working experience\n",
    "experience_reshaped = experience.reshape(-1, 1)  # Reshape for sklearn compatibility\n",
    "model_income_experience = LinearRegression().fit(experience_reshaped, income)\n",
    "beta_0_ie = model_income_experience.intercept_\n",
    "beta_1_ie = model_income_experience.coef_[0]\n",
    "\n",
    "# Print the coefficients for model 2\n",
    "print(f\"Model 2 (Income based on Working Experience): β0 = {beta_0_ie}, β1 = {beta_1_ie}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 for Consumption based on Income model: 0.5815606212788424\n",
      "R^2 for Income based on Working Experience model: 0.6026979398313262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Predictions for R^2 calculation\n",
    "predictions_ci = model_consumption_income.predict(income.reshape(-1, 1))\n",
    "predictions_ie = model_income_experience.predict(experience.reshape(-1, 1))\n",
    "\n",
    "# Calculate R^2 for both models\n",
    "R_squared_ci = r2_score(consumption, predictions_ci)\n",
    "R_squared_ie = r2_score(income, predictions_ie)\n",
    "\n",
    "# Printing R^2 values\n",
    "print(f\"R^2 for Consumption based on Income model: {R_squared_ci}\")\n",
    "print(f\"R^2 for Income based on Working Experience model: {R_squared_ie}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationships identified in these models highlight the influence of income on consumption and the impact of working experience on income. The positive relationships are consistent with economic theories that suggest higher income leads to higher consumption and that more experience typically results in higher earnings due to increased skills and value in the job market.\n",
    "\n",
    "The reason behind the R^2 values not being closer to 1 in both models can be attributed to the complexity of human behavior and economic outcomes, which are influenced by a multitude of factors beyond just income or experience. Factors like personal preferences, economic conditions, education, and the nature of the job can all significantly impact consumption patterns and income levels, making it challenging to capture all variance with a single predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "β0 = 4.528512452357916, β1 = 0.09923934486798955\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import the data\n",
    "data = pd.read_csv('experiment.txt')  # Make sure the path is correct\n",
    "\n",
    "# Prepare the data\n",
    "X = data['x'].values.reshape(-1, 1)\n",
    "y = data['y']\n",
    "\n",
    "# Fit the linear model\n",
    "model = LinearRegression().fit(X, y)\n",
    "beta_0 = model.intercept_\n",
    "beta_1 = model.coef_[0]\n",
    "\n",
    "print(f\"β0 = {beta_0}, β1 = {beta_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.21072725319848384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Predict y values\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Compute R^2\n",
    "r_squared = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"R^2: {r_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. R^2 is closer to 0 than it is to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE(β1): 0.006079555372874688\n",
      "t-statistic for β1: 16.323454394505287\n",
      "p-value for β1: 0.0\n",
      "95% CI for β1: (0.08708023412224017, 0.11139845561373893)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Residuals\n",
    "residuals = y - (beta_0 + beta_1 * X.flatten())\n",
    "\n",
    "# Standard error of the slope (SE(beta_1))\n",
    "n = len(X)\n",
    "X_mean = np.mean(X)\n",
    "SE_beta_1 = np.sqrt(np.sum(residuals**2) / (n - 2)) / np.sqrt(np.sum((X.flatten() - X_mean)**2))\n",
    "\n",
    "# t-statistic for beta_1\n",
    "t_statistic = beta_1 / SE_beta_1\n",
    "\n",
    "# Calculate p-value from t_statistic using scipy (if available)\n",
    "from scipy.stats import t\n",
    "p_value = 2 * (1 - t.cdf(np.abs(t_statistic), df=n-2))\n",
    "\n",
    "# 95% Confidence Interval for beta_1\n",
    "CI_lower = beta_1 - 2 * SE_beta_1\n",
    "CI_upper = beta_1 + 2 * SE_beta_1\n",
    "\n",
    "print(f\"SE(β1): {SE_beta_1}\")\n",
    "print(f\"t-statistic for β1: {t_statistic}\")\n",
    "print(f\"p-value for β1: {p_value}\")\n",
    "print(f\"95% CI for β1: ({CI_lower}, {CI_upper})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Hypothesis Test\n",
    "\n",
    "The t-statistic for β1 is 16.32, and the p-value is 0.0. This indicates a very strong statistical evidence against the null hypothesis, leading us to reject the null hypothesis. The result implies that there is a statistically significant linear relationship between x and y, with the slope (β1) being significantly different from zero. This suggests that changes in x are associated with changes in y.\n",
    "\n",
    "## (d) 95% Confidence Interval for β1\n",
    "\n",
    "The 95% confidence interval for β1 is calculated to be (0.087, 0.111). This interval does not include 0, which further supports the conclusion that β1 is significantly different from 0. However, since the entire interval is below 1, it suggests that while β1 is different from 0, it does not meet the research criterion of β1 ≥ 1 for being meaningfully different from 0 in the context of the research.\n",
    "\n",
    "## (e) Summary of Contradiction and Recommendations\n",
    "\n",
    "There seems to be a contradiction in how we interpret the significance and the meaningful difference of β1. On one hand, the statistical test (part c) strongly rejects the null hypothesis, indicating that β1 is significantly different from 0. On the other hand, the 95% confidence interval (part d) suggests that while β1 is statistically different from 0, it does not reach the threshold of 1 to be considered meaningfully different in the context of this research.\n",
    "\n",
    "This contradiction arises from the difference between statistical significance and practical (or meaningful) significance. Statistical significance (p-value) tells us whether an effect exists, but it does not indicate the size of the effect. The confidence interval helps assess the precision of the estimate and gives a range within which the true value of the parameter is likely to fall, but it does not directly inform about the practical importance of the findings.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "- **Assess Both Statistical and Practical Significance:** We can always consider both statistical significance (through hypothesis testing) and practical significance (magnitude of the effect, as suggested by confidence intervals and the context of the research question).\n",
    "\n",
    "- **Use Confidence Intervals:** Confidence intervals can also provide more information than p-values alone, including the range of plausible values for the parameter and the precision of the estimate.\n",
    "\n",
    "- **Contextual Interpretation:** We can interpret the results within the context of the research or practical application. Even a statistically significant result may not be practically meaningful, depending on the objectives and thresholds defined by the study or domain.\n",
    "\n",
    "While statistical analysis can provide evidence of relationships or effects, the interpretation of these findings should always consider the practical implications and relevance to the specific research context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "duration = np.array([2, 1.8, 3.7, 2.2, 2.1, 2.4, 2.6, 2.8, 3.3, 3.5, 3.7, 3.8, 4.5, 4.7, 4, 4, 1.7, 1.8, 4.9, 4.2, 4.3]).reshape(-1, 1)\n",
    "next_eruption = np.array([50, 57, 55, 47, 53, 50, 62, 57, 72, 62, 63, 70, 85, 75, 77, 70, 43, 48, 70, 79, 72])\n",
    "\n",
    "# Fit the linear model\n",
    "model = LinearRegression().fit(duration, next_eruption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.7490804158751225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(duration)\n",
    "\n",
    "# Compute R^2\n",
    "r_squared = r2_score(next_eruption, predictions)\n",
    "\n",
    "print(f\"R^2: {r_squared}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% prediction interval: (67.47467085165766, 92.45223887800256)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# Assuming the duration of the last eruption was 5 minutes\n",
    "duration_5 = np.array([[5]])\n",
    "\n",
    "# Predict the next eruption time\n",
    "predicted_next = model.predict(duration_5)[0]\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = next_eruption - model.predict(duration)\n",
    "\n",
    "# Standard deviation of the residuals\n",
    "std_dev = np.std(residuals)\n",
    "\n",
    "# Standard error of the forecast\n",
    "se_forecast = std_dev * np.sqrt(1 + 1/len(duration))\n",
    "\n",
    "# Degrees of freedom\n",
    "df = len(duration) - 2\n",
    "\n",
    "# 95% prediction interval\n",
    "t_value = t.ppf(0.975, df)\n",
    "margin_error = t_value * se_forecast\n",
    "\n",
    "lower_bound = predicted_next - margin_error\n",
    "upper_bound = predicted_next + margin_error\n",
    "\n",
    "print(f\"95% prediction interval: ({lower_bound}, {upper_bound})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine if we can see the eruption within 50 minutes based on the prediction interval, compare the 50 minutes to the calculated interval. If 50 minutes is within the interval, it suggests there's uncertainty, and it might be possible to witness the eruption. Otherwise, the prediction would provide a more definitive answer based on whether 50 minutes is less than the lower bound or greater than the upper bound of the interval.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- The R^2 value will indicate how well the linear model fits the data. The R^2 value (0.749) suggests a good fit, meaning the model explains a significant portion of the variability in eruption times.\n",
    "\n",
    "- The 95% prediction interval for a 5-minute duration provides a range within which the next eruption is likely to occur with 95% confidence. This interval accounts for the uncertainty in the prediction.\n",
    "\n",
    "- If the 50-minute mark is within the prediction interval, it implies uncertainty about whether we can see the eruption in time. If the interval suggests the next eruption could occur before or around 50 minutes, planning to witness the eruption might be reasonable. However, this decision also depends on the level of risk we're willing to accept based on the prediction's uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Interpretation of Coefficients in Linear Regression\n",
    "\n",
    "## Part (a): Choice of Encoding for Categorical Feature\n",
    "The preferable encoding method for a categorical feature like fish species, which has no ordinal relationship, is option (2): creating three indicator variables $X_{2}^{A}$, $X_{2}^{B}$, and $X_{2}^{C}$. This method is known as one-hot encoding. The reasoning behind this preference includes:\n",
    "\n",
    "- **Prevents Misinterpretation:** Encoding species as numeric values {1, 2, 3} could imply an ordinal relationship that doesn't exist, potentially misleading the model to interpret species B as \"twice\" species A, which is not meaningful in this context.\n",
    "\n",
    "- **Model Flexibility:** One-hot encoding allows the model to treat each fish species as a separate entity without imposing an artificial order. It enables the model to capture the unique impact of each species on sales independently.\n",
    "\n",
    "- **Interpretability:** Coefficients associated with one-hot encoded variables directly reflect the impact of the presence (or absence) of each category (fish species) on the response variable (sales), making the model's outcomes easier to interpret.\n",
    "\n",
    "## Part (b): Modeling the Weight of Fish on Sales\n",
    "Given the choice of one-hot encoding, the model to relate the weight of the fish (X_{1}) and the species ($X_{2}^{A}$, $X_{2}^{B}$, $X_{2}^{C}$) to sales (Y) would be:\n",
    "\n",
    "$Y = β_{0} + β_{1}X_{1} + β_{2}X_{2}^{A} + β_{3}X_{2}^{B} + β_{4}X_{2}^{C} + \\epsilon$\n",
    "\n",
    "In this model:\n",
    "- $X_{1}$ is the weight of the fish.\n",
    "- $X_{2}^{A}$, $X_{2}^{B}$, and $X_{2}^{C}$ are indicator variables for species A, B, and C, respectively.\n",
    "\n",
    "## Part (c): Interpretation of Coefficients\n",
    "- β0: This is the intercept term. It represents the expected sales when the weight of the fish is 0 and none of the species indicators are active. This term might not have a practical interpretation in this specific context but is necessary for the model.\n",
    "- β1: This coefficient represents the change in sales for a one-unit increase in the weight of the fish, assuming the species remains constant. This reflects the impact of fish weight on sales, regardless of the species.\n",
    "- β2, β3, β4: These coefficients represent the additional effect on sales from being species A, B, and C, respectively, compared to the baseline species (assuming one is treated as baseline if omitted due to collinearity concerns). For example, β2 would be the difference in sales between species A and the baseline species for the same fish weight.\n",
    "\n",
    "Interaction Terms (If Included)\n",
    "\n",
    "If we decide to include interaction terms between weight and species (e.g. $β_{5}X_{1}X_{2}^{A}$), these coefficients would interpret as:\n",
    "- β5 (and similar for other species): Represents how the impact of fish weight on sales changes for species A compared to the baseline. This allows the model to capture differences in how weight influences sales across different species.\n",
    "\n",
    "Case-by-Case Interpretation:\n",
    "\n",
    "- When the fish species is A: The expected sales would be $β_{0} + β_{1}X_{1} + β_{2}$ (assuming species A as the reference and not including interaction terms for simplicity).\n",
    "- When the fish species is B or C: Similar interpretations apply, with the respective β for the species indicating how the baseline sales adjust for that species.\n",
    "\n",
    "The model's coefficients thus provide insights into both the general effect of weight on sales and how this effect varies across different fish species, considering the chosen encoding strategy enhances interpretability by treating species as distinct categories without implying any ordinal relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bias, Variance, and Regularization\n",
    "\n",
    "## Part (a)\n",
    "\n",
    "The bias and variance is related to a model's ability to generalize well to unseen data. A model with high bias pays very little attention to the training data and oversimplifies the model, leading to high error on both training and test data. Conversely, a model with high variance pays too much attention to the training data, capturing noise as well as the underlying data distribution, which results in low training error but high test error.\n",
    "\n",
    "**Large Bias:**\n",
    "\n",
    "A classifier with a large bias typically has a very simple decision boundary, which does not capture the complexity of the data. This is often seen in underfitted models.\n",
    "\n",
    "**Large Variance:**\n",
    "\n",
    "A classifier with a large variance has a complex decision boundary that attempts to fit all the training data points perfectly, often leading to overfitting.\n",
    "\n",
    "In the first set of figures, the leftmost classifier has a simple linear decision boundary, indicative of high bias and potentially high training and test errors. The middle classifier has a more complex boundary that captures more of the data structure but may still have a moderate level of bias. The rightmost classifier shows a very complex, non-linear boundary, indicative of high variance and potential overfitting, where the training error may be low but the test error could be high.\n",
    "\n",
    "To draw an approximate graph demonstrating how training and test error would change over time:\n",
    "\n",
    "- **High Bias Classifier (Leftmost):** Training and test errors start high and remain high as the model is too simple.\n",
    "- **Moderate Bias/Variance Classifier (Middle):** Training error decreases and test error decreases but might increase slightly after some point due to beginning to overfit.\n",
    "- **High Variance Classifier (Rightmost):** Training error decreases to very low, while test error decreases initially but then significantly increases as the model overfits.\n",
    "\n",
    "## Part (b)\n",
    "\n",
    "Regularization techniques like L1 (Lasso) and L2 (Ridge) are used to prevent overfitting by penalizing large coefficients in the model. L1 regularization tends to produce sparser models where some coefficient estimates can be exactly zero, thus performing feature selection. L2 regularization, on the other hand, typically does not reduce coefficients to zero but rather shrinks them.\n",
    "\n",
    "- Figure (a) with the blue line is logistic regression without regularization. The black line is likely the result of L2 regularization since it appears smoother and still retains all features but with reduced effect.\n",
    "- Figure (b) shows a more drastic change from the blue to the black line, potentially eliminating some features (coefficients reduced to zero), which is characteristic of L1 regularization.\n",
    "\n",
    "Thus, Figure (b) is more likely to represent L1 regularization, as the decision boundary changes more significantly, suggesting that some coefficients have been reduced to zero, simplifying the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logistic Regression\n",
    "\n",
    "## (a) Odds and Probability with Given Coefficients\n",
    "\n",
    "Given coefficients and $X_{1}$ = $X_{2}$ = 1, the logistic regression equation is:\n",
    "\n",
    "Logit probability: $\\log(\\frac{P(Y=1)}{1-P(Y=1)}) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2$\n",
    "\n",
    "Substituting values: $\\log(\\frac{P(Y=1)}{1-P(Y=1)}) = 3 + 5(1) - 8(1) = 0$\n",
    "\n",
    "Odds of $Y = 1$ are $e^0 = 1$.\n",
    "\n",
    "Probability is:\n",
    "\n",
    "$P(Y=1) = \\frac{e^{\\log(\\frac{P(Y=1)}{1-P(Y=1)})}}{1 + e^{\\log(\\frac{P(Y=1)}{1-P(Y=1)})}} = \\frac{1}{2} = 0.5$\n",
    "\n",
    "## (b) Change in Log Odds and Odds\n",
    "\n",
    "Increasing $X_{1}$ by 1:\n",
    "\n",
    "Change in log odds: $\\Delta \\log(\\frac{P(Y=1)}{1-P(Y=1)}) = \\beta_1 = 5$.\n",
    "\n",
    "New odds: multiplied by $e^{\\beta_1} = e^5$.\n",
    "\n",
    "Decreasing $X_{2}$ by 1:\n",
    "\n",
    "Change in log odds: $\\Delta \\log(\\frac{P(Y=1)}{1-P(Y=1)}) = -\\beta_2 = 8$.\n",
    "\n",
    "New odds: multiplied by $e^{-\\beta_2} = e^8$, reflecting an increase.\n",
    "\n",
    "## (c) Effect of Increasing or Decreasing Coefficients\n",
    "\n",
    "- Increasing $\\beta_0$: Raises the log odds of $Y=1$ across all values of $X_1$ and $X_2$.\n",
    "- Increasing $\\beta_1$: Increases the probability of $Y=1$ as $X_1$ increases.\n",
    "- Increasing $\\beta_2$: Given $\\beta_2$ is negative, increasing its absolute value makes $Y=1$ less likely as $X_2$ increases.\n",
    "- Decreasing any $\\beta$ has the opposite effect.\n",
    "\n",
    "## (d) Decision Boundary Formulation\n",
    "\n",
    "Decision boundary where logit probability is zero:\n",
    "\n",
    "$3 + 5X_1 - 8X_2 = 0$\n",
    "\n",
    "Solving for $X_2$ in terms of $X_1$:\n",
    "\n",
    "$X_2 = \\frac{5X_1 + 3}{8}$\n",
    "\n",
    "Points on this line have equal odds of being in either category ($Y=0$ or $Y=1$).\n",
    "\n",
    "## (e) Differences in Coefficients Across Models\n",
    "\n",
    "Differences in coefficients when fitting separate models for $X_1$ and $X_2$ can be due to multicollinearity, omitted variable bias, or interaction effects. This discrepancy highlights the importance of including all relevant predictors in the model and considering potential interactions and multicollinearity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
